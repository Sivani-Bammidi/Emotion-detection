{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sivani-Bammidi/Emotion-detection/blob/main/Copy_of_Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"archive.zip\"\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtGJeSMXQJkI",
        "outputId": "d9429216-5b79-4c73-fde1-8753c470729f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "wuvd8UueT1x_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'train'\n",
        "val_dir = 'test'\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(48,48),\n",
        "    batch_size=64,\n",
        "    color_mode=\"grayscale\",\n",
        "    class_mode='categorical')\n",
        "\n",
        "valadition_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(48,48),\n",
        "    batch_size=64,\n",
        "    color_mode=\"grayscale\",\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-V5xzt0Y9Qp",
        "outputId": "13b4d2ee-545d-44d4-a790-f636dd909eed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_model = Sequential()\n",
        "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
        "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "emotion_model.add(Flatten())\n",
        "emotion_model.add(Dense(1024, activation='relu'))\n",
        "emotion_model.add(Dropout(0.5))\n",
        "emotion_model.add(Dense(7, activation='softmax'))"
      ],
      "metadata": {
        "id": "wcRRF_Fvb-B0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001, decay=1e-6),metrics=['accuracy'])\n",
        "emotion_model_info = emotion_model.fit(train_generator,\n",
        "        steps_per_epoch=28709 // 64,\n",
        "        epochs=50,\n",
        "        validation_data=valadition_generator,\n",
        "        validation_steps=7178 // 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn5rD7uyfeIc",
        "outputId": "9fbbedfb-f21b-4a60-c7b1-3f155eeed69e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "448/448 [==============================] - 12s 26ms/step - loss: 1.1841 - accuracy: 0.5537 - val_loss: 1.1890 - val_accuracy: 0.5403\n",
            "Epoch 2/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 1.1579 - accuracy: 0.5649 - val_loss: 1.1762 - val_accuracy: 0.5516\n",
            "Epoch 3/50\n",
            "448/448 [==============================] - 11s 25ms/step - loss: 1.1311 - accuracy: 0.5765 - val_loss: 1.1551 - val_accuracy: 0.5569\n",
            "Epoch 4/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 1.1039 - accuracy: 0.5842 - val_loss: 1.1407 - val_accuracy: 0.5624\n",
            "Epoch 5/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 1.0850 - accuracy: 0.5943 - val_loss: 1.1358 - val_accuracy: 0.5667\n",
            "Epoch 6/50\n",
            "448/448 [==============================] - 11s 25ms/step - loss: 1.0581 - accuracy: 0.6043 - val_loss: 1.1290 - val_accuracy: 0.5693\n",
            "Epoch 7/50\n",
            "448/448 [==============================] - 12s 26ms/step - loss: 1.0423 - accuracy: 0.6110 - val_loss: 1.1184 - val_accuracy: 0.5759\n",
            "Epoch 8/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 1.0167 - accuracy: 0.6215 - val_loss: 1.1010 - val_accuracy: 0.5837\n",
            "Epoch 9/50\n",
            "448/448 [==============================] - 11s 25ms/step - loss: 0.9966 - accuracy: 0.6302 - val_loss: 1.0941 - val_accuracy: 0.5893\n",
            "Epoch 10/50\n",
            "448/448 [==============================] - 11s 25ms/step - loss: 0.9735 - accuracy: 0.6397 - val_loss: 1.0844 - val_accuracy: 0.5918\n",
            "Epoch 11/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.9498 - accuracy: 0.6463 - val_loss: 1.0837 - val_accuracy: 0.5903\n",
            "Epoch 12/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.9305 - accuracy: 0.6579 - val_loss: 1.0747 - val_accuracy: 0.6010\n",
            "Epoch 13/50\n",
            "448/448 [==============================] - 11s 25ms/step - loss: 0.9067 - accuracy: 0.6647 - val_loss: 1.0724 - val_accuracy: 0.6055\n",
            "Epoch 14/50\n",
            "448/448 [==============================] - 12s 26ms/step - loss: 0.8864 - accuracy: 0.6710 - val_loss: 1.0729 - val_accuracy: 0.6013\n",
            "Epoch 15/50\n",
            "448/448 [==============================] - 12s 26ms/step - loss: 0.8614 - accuracy: 0.6808 - val_loss: 1.0739 - val_accuracy: 0.6021\n",
            "Epoch 16/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.8404 - accuracy: 0.6905 - val_loss: 1.0609 - val_accuracy: 0.6095\n",
            "Epoch 17/50\n",
            "448/448 [==============================] - 12s 26ms/step - loss: 0.8209 - accuracy: 0.6969 - val_loss: 1.0597 - val_accuracy: 0.6127\n",
            "Epoch 18/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.7947 - accuracy: 0.7075 - val_loss: 1.0612 - val_accuracy: 0.6099\n",
            "Epoch 19/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.7709 - accuracy: 0.7181 - val_loss: 1.0623 - val_accuracy: 0.6136\n",
            "Epoch 20/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.7491 - accuracy: 0.7257 - val_loss: 1.0551 - val_accuracy: 0.6208\n",
            "Epoch 21/50\n",
            "448/448 [==============================] - 12s 26ms/step - loss: 0.7320 - accuracy: 0.7322 - val_loss: 1.0538 - val_accuracy: 0.6180\n",
            "Epoch 22/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.7154 - accuracy: 0.7413 - val_loss: 1.0691 - val_accuracy: 0.6203\n",
            "Epoch 23/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.6924 - accuracy: 0.7470 - val_loss: 1.0684 - val_accuracy: 0.6170\n",
            "Epoch 24/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.6661 - accuracy: 0.7576 - val_loss: 1.0694 - val_accuracy: 0.6274\n",
            "Epoch 25/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.6478 - accuracy: 0.7665 - val_loss: 1.0754 - val_accuracy: 0.6225\n",
            "Epoch 26/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.6237 - accuracy: 0.7762 - val_loss: 1.0908 - val_accuracy: 0.6246\n",
            "Epoch 27/50\n",
            "448/448 [==============================] - 12s 26ms/step - loss: 0.6044 - accuracy: 0.7826 - val_loss: 1.0702 - val_accuracy: 0.6249\n",
            "Epoch 28/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.5852 - accuracy: 0.7886 - val_loss: 1.0977 - val_accuracy: 0.6214\n",
            "Epoch 29/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.5711 - accuracy: 0.7925 - val_loss: 1.0792 - val_accuracy: 0.6263\n",
            "Epoch 30/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.5444 - accuracy: 0.8045 - val_loss: 1.1091 - val_accuracy: 0.6246\n",
            "Epoch 31/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.5364 - accuracy: 0.8069 - val_loss: 1.0988 - val_accuracy: 0.6249\n",
            "Epoch 32/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.5142 - accuracy: 0.8129 - val_loss: 1.0991 - val_accuracy: 0.6270\n",
            "Epoch 33/50\n",
            "448/448 [==============================] - 11s 25ms/step - loss: 0.5041 - accuracy: 0.8172 - val_loss: 1.1142 - val_accuracy: 0.6283\n",
            "Epoch 34/50\n",
            "448/448 [==============================] - 11s 25ms/step - loss: 0.4776 - accuracy: 0.8265 - val_loss: 1.1095 - val_accuracy: 0.6239\n",
            "Epoch 35/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.4680 - accuracy: 0.8293 - val_loss: 1.1276 - val_accuracy: 0.6233\n",
            "Epoch 36/50\n",
            "448/448 [==============================] - 12s 26ms/step - loss: 0.4519 - accuracy: 0.8364 - val_loss: 1.1383 - val_accuracy: 0.6274\n",
            "Epoch 37/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.4361 - accuracy: 0.8433 - val_loss: 1.1421 - val_accuracy: 0.6302\n",
            "Epoch 38/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.4172 - accuracy: 0.8506 - val_loss: 1.1465 - val_accuracy: 0.6256\n",
            "Epoch 39/50\n",
            "448/448 [==============================] - 11s 25ms/step - loss: 0.4066 - accuracy: 0.8536 - val_loss: 1.1669 - val_accuracy: 0.6260\n",
            "Epoch 40/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.3978 - accuracy: 0.8569 - val_loss: 1.1819 - val_accuracy: 0.6289\n",
            "Epoch 41/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.3851 - accuracy: 0.8626 - val_loss: 1.1728 - val_accuracy: 0.6283\n",
            "Epoch 42/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.3667 - accuracy: 0.8683 - val_loss: 1.1872 - val_accuracy: 0.6263\n",
            "Epoch 43/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.3684 - accuracy: 0.8651 - val_loss: 1.1873 - val_accuracy: 0.6244\n",
            "Epoch 44/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.3536 - accuracy: 0.8732 - val_loss: 1.2014 - val_accuracy: 0.6336\n",
            "Epoch 45/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.3383 - accuracy: 0.8778 - val_loss: 1.2233 - val_accuracy: 0.6260\n",
            "Epoch 46/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.3350 - accuracy: 0.8792 - val_loss: 1.2217 - val_accuracy: 0.6254\n",
            "Epoch 47/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.3220 - accuracy: 0.8843 - val_loss: 1.2433 - val_accuracy: 0.6225\n",
            "Epoch 48/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.3217 - accuracy: 0.8865 - val_loss: 1.2132 - val_accuracy: 0.6260\n",
            "Epoch 49/50\n",
            "448/448 [==============================] - 12s 26ms/step - loss: 0.3064 - accuracy: 0.8909 - val_loss: 1.2506 - val_accuracy: 0.6274\n",
            "Epoch 50/50\n",
            "448/448 [==============================] - 11s 24ms/step - loss: 0.2982 - accuracy: 0.8939 - val_loss: 1.2510 - val_accuracy: 0.6311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_model.save('model.h5')"
      ],
      "metadata": {
        "id": "GM94vWNDmx9f"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "emotion_model = load_model('model.h5')"
      ],
      "metadata": {
        "id": "nVg5wc46nhjU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emotion_analysis(emotions):\n",
        "  objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "  y_pos = np.arange(len(objects))\n",
        "\n",
        "  plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
        "  plt.xticks(y_pos, objects)\n",
        "  plt.ylabel('percentage')\n",
        "  plt.title('emotion')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "UDUHKCMAoKwu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript(''' async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "oZY84Qe9qhiO"
      },
      "execution_count": 38,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}